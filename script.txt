Bloco 0 — Download do dataset (célula inicial)
Nesta etapa baixamos os dados do Kaggle usando a ferramenta kagglehub. O comando retorna o caminho local onde os arquivos CSV foram salvos — a partir daqui carregamos o dataset para análise.

Bloco 1 — Imports, configuração e carregamento inicial
Importamos bibliotecas essenciais: pandas, numpy, matplotlib, seaborn, sklearn e imblearn. Configuramos estilos de plot e listamos os arquivos CSV no diretório. Em seguida lemos o CSV principal para iniciar a análise.

Bloco 2 — 1. Visão geral do dataset
Fizemos uma visão geral: número de linhas/colunas, primeiras linhas, tipos de dados e estatísticas descritivas. Isso dá uma ideia da qualidade e escala dos dados.

Bloco 3 — Colunas com valores faltantes
Verificamos colunas com dados faltantes. Se houverem, avalio proporção e remover ou tratar.

Bloco 4 — Distribuição do alvo (diagnosed_diabetes)
Analisamos a distribuição da variável alvo 'diagnosed_diabetes' com contagens absolutas e relativas, além de gráficos. Calculamos a razão de balanceamento para entender se é necessário aplicar técnicas para classes desbalanceadas.

Bloco 5 — Diagnóstico por gênero
Fizemos crosstab por gênero para ver diferenças na prevalência. Visualmente e estatisticamente avaliamos se gender tem poder discriminatório para o target.
E identificamos que a proporção é praticamente a mesma, optamos em remover a coluna gender para simplificar o modelo sem perda de poder preditivo.

Bloco 6 — Idade por diagnóstico
Aqui comparamos a distribuição de idade entre os grupos com e sem diagnóstico. Observa-se deslocamento nas medianas — idade costuma ser preditiva em doenças crônicas.

Bloco 7 — Diagnóstico por etnia
Analisamos proporções de diabetes por etnia para verificar variabilidade. Se proporções forem muito similares, a feature possuí  baixo poder discriminatório, optamos em remover a coluna gender para simplificar o modelo sem perda de poder preditivo.

Bloco 8 — Teste qui-quadrado para etnia
Rodamos o teste qui‑quadrado para confirmar se diferenças por etnia são estatisticamente significativas. Um p-value alto indica ausência de diferença relevante.

Bloco 9 — Cálculo de correlações (top correlacionadas)
Calculamos a correlação entre variáveis numéricas e o target, ordenando pelas maiores correlações em módulo. Isso destaca features mais correlacionadas com diagnóstico.

Bloco 10 — Menos correlacionadas / análise de correlações fracas
Mostramos as variáveis menos correlacionadas e quantifico variáveis com correlação muito fraca (|r| < 0.1), para considerar remoção.

Bloco 11 — Seleção e remoção de variáveis
Com base nas correlações, poder discriminatório e multicolinearidade, definimos uma lista de variáveis a remover.  Removemos as existentes do dataset e imprimo dimensões antes e depois para registro.

Bloco 12 — Resumo das remoções justificadas (markdown)
Resumimos as remoções com justificativas: quais variáveis foram removidas por baixa correlação, quais por baixo poder discriminatório e quais por multicolinearidade — assim simplificamos o espaço de features.

Bloco 13 — Pré‑processamento: separação X/y e split
Separamos features (X) e target (y), removendo colunas não usadas. Identifico colunas categóricas e numéricas e faço split estratificado treino/teste 80/20."

Bloco 14 — Preparação com diferentes encoders e scalers
Criamos cinco variantes de pré‑processamento: LabelEncoder puro, LabelEncoder + StandardScaler, LabelEncoder + MinMax, OneHotEncoder + StandardScaler e OneHotEncoder + MinMax. Isso permite comparar impacto de encoding e scaling nos modelos.

Bloco 15 — Oversampling com SMOTE
Como as classes podem estar desbalanceadas, aplicamos SMOTE nas versões preparadas para gerar exemplos sintéticos da classe minoritária.  Criamos splits para versões com e sem oversampling."

Bloco 16 — Funções para otimização de hiperparâmetros
Implementamos funções simples para escolher hiperparâmetros: melhor K para KNN e melhor max_depth para Decision Tree e Random Forest testando valores de 3 a 8 com base no erro.

Bloco 17 — Experimentos com KNN
Rodamos KNN em todas as variantes de pré‑processamento e com/sem SMOTE, usando a função que escolhe o K ideal. Registro accuracy, precision, recall e F1 de cada experimento.

Bloco 18 — Experimentos com SVM (LinearSVC)
Executamos LinearSVC em todas as variantes. Como SVM é sensível à escala e encoding, comparar versões com Standard e MinMax é importante para desempenho.

Bloco 19 — Experimentos com Decision Tree
Decision Tree foi testada com busca por melhor max_depth. Árvore é menos sensível a escala, mas validação do max_depth evita overfitting.

Bloco 20 — Experimentos com Random Forest
Random Forest combina várias árvores; também busco o melhor max_depth e registro métricas. É geralmente robusto e costuma ter bom F1 em problemas tabulares.

Bloco 21 — Experimentos com Logistic Regression
Logistic Regression é baseline interpretável. Testo nas mesmas variantes para comparar performance contra modelos mais complexos.

Bloco 22 — Análise comparativa e criação do DataFrame de resultados
Agregamos todos os resultados em um DataFrame para análise comparativa. Localizo os melhores por Accuracy, Precision, Recall e F1 — isso facilita a decisão sobre a melhor combinação.

Bloco 23 — Visualizações dos resultados
Criamos visualizações: boxplots das métricas por modelo, heatmap de correlação entre métricas, heatmap de F1 por modelo x encoder e comparação média por modelo com e sem oversampling para interpretar estabilidade e impacto do SMOTE.

Bloco 24 — Relatório final e conclusões
No relatório final resumimos estatísticas, performance média por modelo e encoder, impacto do oversampling e top‑5 experimentos por F1. Concluo indicando o melhor modelo e pré‑processamento recomendados, além de limitações e próximos passos.

Encerramento (últimos 20–30s)
Aqui encerramos o  notebook contém todo o código e resultados.
Muito Obrigado.

